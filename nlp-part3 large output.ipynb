{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import collections, math, random, sys\nimport torch\ntorch.set_default_device('cuda') # uncomment this line to use GPU\nsys.path.append('/kaggle/input/nlphw1')\nfrom utils import *\ntraindata = read_mono('/kaggle/input/nlphw1/data/large', delim='')\ndevdata = read_mono('/kaggle/input/nlphw1/data/dev', delim='')\ntestdata = read_mono('/kaggle/input/nlphw1/data/test', delim='')\n\nvocab = Vocab()\nfor words in traindata:\n    vocab |= words","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T19:17:57.465172Z","iopub.execute_input":"2023-09-21T19:17:57.465595Z","iopub.status.idle":"2023-09-21T19:18:02.192251Z","shell.execute_reply.started":"2023-09-21T19:17:57.465557Z","shell.execute_reply":"2023-09-21T19:18:02.190691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits = torch.normal(mean=0, std=0.01, \n                      size=(len(vocab),), \n                      requires_grad=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:18:02.194424Z","iopub.execute_input":"2023-09-21T19:18:02.194909Z","iopub.status.idle":"2023-09-21T19:18:02.223029Z","shell.execute_reply.started":"2023-09-21T19:18:02.194876Z","shell.execute_reply":"2023-09-21T19:18:02.222105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass myLSTM(torch.nn.Module):\n    def __init__(self,vocab,dim):\n        super().__init__()\n        self.vocab = vocab\n        self.dim = dim\n        self.lstm = torch.nn.LSTMCell(len(vocab),dim) #should I pass in hidden_dim\n        #self.output = torch.nn.Linear(hidden_dim,len(vocab))\n        self.linear = torch.nn.Linear(d,len(vocab))\n        self.log_softmax = torch.nn.LogSoftmax(dim=0)\n\n    def start(self):\n        hidden_state = torch.zeros(self.dim)\n        cell_state = torch.zeros(self.dim)\n        return hidden_state, cell_state\n    \n    def step(self, state, num):\n        input_step = torch.nn.functional.one_hot(torch.tensor(num),num_classes = len(self.vocab)).float()\n        hidden_state, cell_state = self.lstm(input_step,state)\n        y = self.linear(hidden_state)\n        y = self.log_softmax(y)\n        return (hidden_state,cell_state), y\n    \n    def forward(self, full_seq):\n        hidden_state = torch.zeros(self.dim)\n        cell_state = torch.zeros(self.dim)\n        output = torch.empty((full_seq.shape[0],len(vocab)))\n        for t, x_t in enumerate(full_seq):\n            hidden_state, cell_state = self.lstm(x_t, (hidden_state, cell_state))\n            y = self.linear(hidden_state)\n            y = self.log_softmax(y)\n            output[t] = y\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:18:02.225044Z","iopub.execute_input":"2023-09-21T19:18:02.225930Z","iopub.status.idle":"2023-09-21T19:18:02.242105Z","shell.execute_reply.started":"2023-09-21T19:18:02.225888Z","shell.execute_reply":"2023-09-21T19:18:02.240902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, correct_data):\n    total = 0\n    correct = 0\n    for i, sentance in enumerate(correct_data):\n        prediction = []\n        q = model.start()\n        for j,letter in enumerate(sentance[:-1]): #skip eos\n            rnn_letter = model.vocab.numberize(letter)\n            q, probs = model.step(q,rnn_letter)\n            index = model.vocab.numberize(sentance[j+1])\n            prediction = torch.argmax(probs)\n            if prediction == index:\n                correct += 1\n            total += 1\n    return correct / total","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:18:02.244239Z","iopub.execute_input":"2023-09-21T19:18:02.244653Z","iopub.status.idle":"2023-09-21T19:18:02.254767Z","shell.execute_reply.started":"2023-09-21T19:18:02.244620Z","shell.execute_reply":"2023-09-21T19:18:02.253622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = 512\nmodel = myLSTM(vocab,d)\no = torch.optim.Adam(model.parameters(),lr=2e-3)\nprev_score = 0\nepochs = 10\nfor epoch in range(epochs): #do 5, now 10 epochs\n    random.shuffle(traindata)\n    for i, sentance in enumerate(traindata):\n        log_sum = 0\n        predict_probs = []\n        q = model.start()\n        for j, letter in enumerate(sentance[:-1]): #no EOS\n            rnn_letter = model.vocab.numberize(letter)\n            q, probs = model.step(q, rnn_letter)\n            index = model.vocab.numberize(sentance[j+1])\n            predict_prob = probs[index]\n            predict_probs.append(predict_prob)\n        sum_logs = sum(predict_probs)\n        loss = sum_logs * -1 #want to maximize not minimize\n        o.zero_grad()\n        loss.backward()\n        o.step()\n    print(\"Finished epoch\", epoch)\n    cur_score = validate(model,devdata)\n    print(\"Accuracy: {:3f}%\".format(cur_score * 100))\n    if cur_score < prev_score:\n        for k in o.param_groups:\n            k['lr'] /=2\n    prev_score = cur_score","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:18:02.256422Z","iopub.execute_input":"2023-09-21T19:18:02.257141Z","iopub.status.idle":"2023-09-21T19:18:03.295582Z","shell.execute_reply.started":"2023-09-21T19:18:02.257106Z","shell.execute_reply":"2023-09-21T19:18:03.293505Z"},"trusted":true},"execution_count":null,"outputs":[]}]}